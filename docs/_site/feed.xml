<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://uwrealitylab.github.io/xrcapstone21sp-team4/feed.xml" rel="self" type="application/atom+xml" /><link href="https://uwrealitylab.github.io/xrcapstone21sp-team4/" rel="alternate" type="text/html" /><updated>2021-04-15T23:37:15-07:00</updated><id>https://uwrealitylab.github.io/xrcapstone21sp-team4/feed.xml</id><title type="html">GuitXR</title><subtitle>Augmented Reality project for the University of Washington AR/VR Capstone. We are building a mixed reality guitar assistor that enables guitarists to visualize chords and tabs in front of them and on their instruments.</subtitle><entry><title type="html">Week 2: Basics</title><link href="https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/15/Week2-Basics.html" rel="alternate" type="text/html" title="Week 2: Basics" /><published>2021-04-15T21:00:00-07:00</published><updated>2021-04-15T21:00:00-07:00</updated><id>https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/15/Week2-Basics</id><content type="html" xml:base="https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/15/Week2-Basics.html">&lt;p&gt;Here is the week 2 report on the GuitXR project.&lt;br /&gt;
We all collaborated on finishing our PRD (look at the PRD link above) and this was the first week of prototyping a proof of concept.&lt;br /&gt;
We each worked on a different core feature in seperate repositories.&lt;/p&gt;

&lt;p&gt;Here are the four glitch projects: &lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Kirit: &lt;a href=&quot;https://glitch.com/edit/#!/capstone-leap&quot;&gt;TensorFLow.js for basic object detection&lt;/a&gt; &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Rishabh: &lt;a href=&quot;https://glitch.com/edit/#!/flash-accurate-soda&quot;&gt;Image, Barcode, Pattern trackers&lt;/a&gt; &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Sam: &lt;a href=&quot;https://glitch.com/edit/#!/rapid-prototype-vr&quot;&gt;Displaying tabs to the user&lt;/a&gt; &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Maxime: &lt;a href=&quot;https://glitch.com/edit/#!/remixed-capstone-leap-audio?path=index.html%3A1%3A0&quot;&gt;Tapping into the Magic Leap’s audio feed&lt;/a&gt; &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now lets get more indepth on what each member did this week…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kirit&lt;/strong&gt; worked on evaluating the feasibility of WebXR and AFrame for tracking the guitar fretboard through a Magic Leap camera.
He tried to port over the popular web framework AR.js but ran into many issues because the framework is designed to support AR through mobile devices and is incompatible with running on the Magic Leap’s Helio browser.&lt;/p&gt;

&lt;p&gt;He was able to access the magic leap camera feed, and use TensorFlow.js to perform basic object detection. 
However, attempting to use JavaScript marker tracking libraries proved to have issues running on the device, and there does not seem to be a good implementation that gives pose information in the Magic Leap’s camera space.&lt;br /&gt;
&lt;img src=&quot;/xrcapstone21sp-team4/images/kirit-demo-week-2.png&quot; alt=&quot;Kirit-demo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rishabh&lt;/strong&gt; worked on finding ways for marker tracking that works with A-frame. We plan to have our first version of the application using marker tracking and we will make using CV for fret detection a stretch goal.&lt;/p&gt;

&lt;p&gt;He experimented with three different methods of tracking:-&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Image tracker&lt;/li&gt;
  &lt;li&gt;Barcode tracker&lt;/li&gt;
  &lt;li&gt;Pattern tracker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of the three were part of the arjs library. We were able to get a working example with the pattern tracker. 
To use: -&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Open link using phone or computer.&lt;/li&gt;
  &lt;li&gt;Search for the HIRO marker on google and show it to the camera that you are using.&lt;/li&gt;
  &lt;li&gt;A box should be overlaid on the marker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This showed success to our idea for using markers on the guitar. 
However, I ran into a problem where MagicLeap does not support arjs yet. Thus, when we open the demo from a magic leap browser it shows us an error. Kirit tried to work around this but it does not seem possible at the moment.&lt;br /&gt;
&lt;img src=&quot;/xrcapstone21sp-team4/images/hiro-week-2.png&quot; alt=&quot;Rish-demo&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sam&lt;/strong&gt; worked on tabs being displayed to the user. Made script that allows tab images to follow the user as they move and maintians a reasonable distance. This allows the tab to be available at a glance, but looking down at the fretboard will not objstruct the player’s vision.&lt;br /&gt;
 The next steps will be to incorporate multiple tabs to cycle through, indicating a chord progression to the user.&lt;br /&gt;
 &lt;img src=&quot;/xrcapstone21sp-team4/images/sam-week-2.png&quot; alt=&quot;sam-demo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Early this week &lt;strong&gt;Maxime&lt;/strong&gt; looked through the documentation for the Magic Leap, set up the device, and realized to test while wearing the device he would need to buy contact lenses. Later he examined the feasibility of audio capture in WebXR / A-Frame using the Magic Leap’s built-in microphone array. Max found these two pages that looked promising for a potential switch to Unity if we cannot get AR.js working in WebXR / A-Frame.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.magicleap.com/en-us/learn/guides/sdk-example-audio-capture&quot;&gt;Audio Capture Example  Magic Leap&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.magicleap.com/en-us/learn/guides/audio-capture-snippet-unity&quot;&gt;Audio Capture Snippet  Unity  Magic Leap&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Kirit shared this page with him for future work on chord recognition.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/connormcl/chord_recognizer&quot;&gt;connormcl/chord_recognizer: Feedforward neural network for live guitar chord recognition (Python + TensorFlow) (github.com)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And Max remixed his Glitch project to take input from the microphones instead of the camera.&lt;/p&gt;

&lt;p&gt;He will continue working over the weekend on the proof of concept to include logic on recognizing peaks/dips in the audio signal. Max is looking forward to the Glitch examples John promised to send demonstrating microphone control on the Magic Leap.&lt;br /&gt;
 &lt;img src=&quot;/xrcapstone21sp-team4/images/max-week-2.png&quot; alt=&quot;max-demo&quot; width=&quot;420px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Summary of &lt;strong&gt;next steps&lt;/strong&gt;:&lt;br /&gt;
Make final determination if we need to switch to unity to continue our project. &lt;br /&gt;
Explore more marker tracking methods and find a feasible one by next week. &lt;br /&gt;
Build out the tabs and chord progression system for the user. &lt;br /&gt;
Detect pitch of audio feed from headset.&lt;/p&gt;

&lt;p&gt;Summary of our &lt;strong&gt;blockers&lt;/strong&gt;:&lt;br /&gt;
Lack of support for the magic leap in VR/AR libraries, especially AR.js and marker libraries.&lt;/p&gt;</content><author><name></name></author><summary type="html">Here is the week 2 report on the GuitXR project. We all collaborated on finishing our PRD (look at the PRD link above) and this was the first week of prototyping a proof of concept. We each worked on a different core feature in seperate repositories.</summary></entry><entry><title type="html">Project Pitch Presentation</title><link href="https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/13/Project-Pitch-Slides.html" rel="alternate" type="text/html" title="Project Pitch Presentation" /><published>2021-04-13T21:00:00-07:00</published><updated>2021-04-13T21:00:00-07:00</updated><id>https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/13/Project-Pitch-Slides</id><content type="html" xml:base="https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/13/Project-Pitch-Slides.html">&lt;p&gt;Our team gave a pitch presentation to the class. Here is a link to the presentation.
 &lt;a href=&quot;https://docs.google.com/presentation/d/1JLyVdGJaDaF8JNFd2yahqoMeULkVKW9H4muH54SHjuw/edit?usp=sharing&quot;&gt;Presentation Link&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Our team gave a pitch presentation to the class. Here is a link to the presentation. Presentation Link</summary></entry><entry><title type="html">Week 1: Liftoff</title><link href="https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/08/Week1-Liftoff.html" rel="alternate" type="text/html" title="Week 1: Liftoff" /><published>2021-04-08T21:00:00-07:00</published><updated>2021-04-08T21:00:00-07:00</updated><id>https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/08/Week1-Liftoff</id><content type="html" xml:base="https://uwrealitylab.github.io/xrcapstone21sp-team4/2021/04/08/Week1-Liftoff.html">&lt;p&gt;Welcome to our week 1 progress update, our team is excited to get started working on GuitXR! 
We spent the majority of the week honing down on the idea and discussing the approaches and technologies we could utilize to create an augmented reality tool for empowering guitarists.&lt;/p&gt;

&lt;p&gt;In addition, Maxime and Samuel went ahead and built this website, while Kirit and Rishabh got a head start on the project by tackling the problem of mapping a physical guitar fretboard and strings.&lt;/p&gt;

&lt;p&gt;The immediate goals we are working towards:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Visualize music chords and tabs in front of a user so they can play any song without needing to commit it to memory or bury their head in note sheets. These will ideally use some form of pitch recognition to move the tabs forward when they are played correctly, and have an easy way of selecting or importing songs.&lt;/li&gt;
  &lt;li&gt;Highlight which strings to pluck on the fretboard of a physical guitar along with visual handshape indicators, making learning new songs easier. We hope to make this an invaluable guitar teaching tool for both beginners and experienced guitarists.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core technical challenge we anticipate is accurately mapping out the position of each string and each fret on a moving physical guitar in realtime while it is held at an angle relative to the camera.
Here are some of our early stage findings (mostly put together by Rishabh): 
&lt;a href=&quot;https://docs.google.com/document/d/1TyTIryU55h1wBYwHiKqDRSgBX48e7zCjgk8D1Dp38I8/edit?usp=sharing&quot;&gt;Week 1 Computer Vision Fretboard Detection Techniques&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are looking to spend next week further experimenting with techniques to achieve this (Computer Vision, markers, external trackers etc) as well as potentially getting an early stage prototype out.&lt;/p&gt;

&lt;p&gt;Some reference material:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;https://ensiwiki.ensimag.fr/index.php?title=GuitAR_Learning_Assitant_in-Augmented_Reality&lt;/li&gt;
  &lt;li&gt;https://github.com/paulden/guitar-fingering-recognition&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Welcome to our week 1 progress update, our team is excited to get started working on GuitXR! We spent the majority of the week honing down on the idea and discussing the approaches and technologies we could utilize to create an augmented reality tool for empowering guitarists.</summary></entry></feed>